GRU:
  data_path: "dataset/wind_dataset.csv"
  timestep: 1  # 时间步长，就是利用多少时间窗口
  feature_size: 1  # 每个步长对应的特征数量
  hidden_size: 256  # GRU 隐藏层大小
  num_layers: 2  # GRU 的层数
  output_size: 1  # 由于是单输出任务，最终输出层大小为 1，预测未来 1 个时刻数据
  epochs: 10  # 迭代轮数
  batch_size: 32  # 批次大小
  learning_rate: 3e-4  # 学习率
  best_loss: 0  # 记录损失
  model_name: "GRU"  # 模型名称
  save_path: f"saved_models/{model_name}.pth"
LSTM:
  Univariate_Single_Output:
    data_path: "dataset/wind_dataset.csv"
    timestep: 1  # 时间步长，就是利用多少时间窗口
    feature_size: 1  # 每个步长对应的特征数量
    num_layers: 2  # LSTM 的层数
    hidden_size: 256  # 隐藏层大小
    output_size: 1  # 由于是单输出任务，最终输出层大小为 1，预测未来 1 个时刻的目标值
    split_ratio: 0.8  # 训练测试数据分割比例
    target_index: 0  # 预测特征的列索引
    epochs: 10  # 迭代轮数
    batch_size: 32  # 批次大小
    learning_rate: 3e-4  # 学习率
    best_loss: 0  # 记录损失
    model_name: "LSTM-Univariate-SingleOutput-V1"  # 模型名称
    save_path: f"saved_models/{model_name}.pth"  # 最优模型保存路径
  Multivariate_Single_Output:
    data_path: "dataset/wind_dataset.csv"
    timestep: 20  # 时间步长，就是利用多少时间窗口
    feature_size: 8  # 每个步长对应的特征数量
    num_layers: 2  # lstm 的层数
    hidden_size: 256  # 隐藏层大小
    output_size: 1  # 由于是单输出任务，最终输出层大小为 1，预测未来 1 个时刻的目标值
    split_ratio: 0.8  # 训练测试数据分割比例
    target_index: None  # 预测特征的列索引
    epochs: 10  # 迭代轮数
    batch_size: 32  # 批次大小
    learning_rate: 3e-4  # 学习率
    best_loss: 0  # 记录损失
    model_name: "Config_MultiVariate_SingleOutput"  # 模型名称
    save_path: f"saved_models/{model_name}.pth"  # 最优模型保存路径
  Multivariate_Multi_Output:
    data_path: "dataset/wind_dataset.csv"
    timestep: 20  # 时间步长，就是利用多少时间窗口
    feature_size: 8  # 每个步长对应的特征数量
    num_layers: 2  # lstm 的层数
    hidden_size: 256  # 隐藏层大小
    output_size: 2  # 由于是单输出任务，最终输出层大小为 1，预测未来 1 个时刻的目标值
    split_ratio: 0.8  # 训练测试数据分割比例
    target_index: 0  # 预测特征的列索引
    epochs: 10  # 迭代轮数
    batch_size: 32  # 批次大小
    learning_rate: 1e-4  # 学习率
    best_loss: 0  # 记录损失
    model_name: "TODO"  # 模型名称
    save_path: f"saved_models/{model_name}.pth"  # 最优模型保存路径
BiLSTM:
  data_path: "dataset/wind_dataset.csv"
  timestep: 1  # 时间步长，就是利用多少时间窗口
  feature_size: 1  # 每个步长对应的特征数量
  num_layers: 2  # 网络的层数
  hidden_size: 256  # 网络隐藏层大小
  output_size: 1  # 预测未来 n 个时刻数据
  epochs: 10  # 迭代轮数
  batch_size: 32  # 批次大小
  learning_rate: 3e-4  # 学习率
  best_loss: 0  # 记录损失
  model_name: "BiLSTM"  # 模型名称
  save_path: f"saved_models/{model_name}.pth"
CNN_Conv1d:
  data_path: "dataset/wind_dataset.csv"
  timestep: 20  # 时间步长，就是利用多少时间窗口
  feature_size: 1  # 每个步长对应的特征数量
  out_channels: [10, 20, 30]  # 卷积输出通道
  output_size: 1  # 预测未来 n 个时刻数据
  epochs: 10  # 迭代轮数
  batch_size: 32  # 批次大小
  learning_rate: 3e-4  # 学习率
  best_loss: 0  # 记录损失
  model_name: "CNN-Conv1d"  # 模型名称
  save_path: f"saved_models/{model_name}.pth"
CNN_Conv2d:
  data_path: "dataset/wind_dataset.csv"
  timestep: 20  # 时间步长，就是利用多少时间窗口
  feature_size: 1  # 每个步长对应的特征数量
  out_channels: [10, 20, 30]  # 卷积输出通道
  output_size: 1  # 预测未来 n 个时刻数据
  epochs: 10  # 迭代轮数
  batch_size: 32  # 批次大小
  learning_rate: 3e-4  # 学习率
  best_loss: 0  # 记录损失
  model_name: "CNN-Conv2d"  # 模型名称
  save_path: f"saved_models/{model_name}.pth"
CNN_LSTM_Attention:
  data_path: "dataset/wind_dataset.csv"
  timestep: 20  # 时间步长，就是利用多少时间窗口
  feature_size: 1  # 每个步长对应的特征数量
  num_layers: 2  # LSTM 网络的层数
  num_heads: 1  # 注意力机制头的数量
  hidden_size: 256  # 网络隐藏层大小
  out_channels: 50  # CNN 输出通道
  output_size: 1  # 预测未来 n 个时刻数据
  epochs: 10  # 迭代轮数
  batch_size: 32  # 批次大小
  learning_rate: 3e-4  # 学习率
  best_loss: 0  # 记录损失
  model_name: "CNN-LSTM-Attention"  # 模型名称
  save_path: f"saved_models/{model_name}.pth"
LSTM_CNN:
  data_path: "dataset/wind_dataset.csv"
  timestep: 20  # 时间步长，就是利用多少时间窗口
  feature_size: 1  # 每个步长对应的特征数量
  num_layers: 2  # 网络的层数
  hidden_size: 256  # 网络隐藏层大小
  out_channels: 50  # CNN输出通道
  output_size: 1  # 预测未来 n 个时刻数据
  epochs: 10  # 迭代轮数
  batch_size: 32  # 批次大小
  learning_rate: 3e-4  # 学习率
  best_loss: 0  # 记录损失
  model_name: "LSTM-CNN"  # 模型名称
  save_path: f"saved_models/{model_name}.pth"
MLP:
  data_path: "dataset/wind_dataset.csv"
  timestep: 20  # 时间步长，就是利用多少时间窗口
  feature_size: 1  # 每个步长对应的特征数量
  hidden_size: [32, 64, 32]  # 网络隐藏层大小
  output_size: 1  # 预测未来 n 个时刻数据
  epochs: 10  # 迭代轮数
  batch_size: 32  # 批次大小
  learning_rate: 3e-4  # 学习率
  best_loss: 0  # 记录损失
  model_name: "MLP"  # 模型名称
  save_path: f"saved_models/{model_name}.pth"
RNN:
  data_path: "dataset/wind_dataset.csv"
  timestep: 1  # 时间步长，就是利用多少时间窗口
  feature_size: 1  # 每个步长对应的特征数量
  num_layers: 2  # 网络的层数
  hidden_size: 256  # 网络隐藏层大小
  output_size: 1  # 预测未来 n 个时刻数据
  epochs: 10  # 迭代轮数
  batch_size: 32  # 批次大小
  learning_rate: 3e-4  # 学习率
  best_loss: 0  # 记录损失
  model_name: "RNN"  # 模型名称
  save_path: f"saved_models/{model_name}.pth"
Seq2Seq_LSTM:
  data_path: "dataset/wind_dataset.csv"
  timestep: 20  # 时间步长，就是利用多少时间窗口
  feature_size: 8  # 每个步长对应的特征数量
  num_layers: 2  # 网络的层数
  hidden_size: 256  # 网络隐藏层大小
  output_size: 2  # 预测未来 n 个时刻数据
  epochs: 10  # 迭代轮数
  batch_size: 32  # 批次大小
  learning_rate: 3e-4  # 学习率
  best_loss: 0  # 记录损失
  model_name: "LSTM-Seq2Seq"  # 模型名称
  save_path: f"saved_models/{model_name}.pth"
TCN:
  data_path: "data/{}.csv"
  timestep: 20  # 时间步长，就是利用多少时间窗口
  feature_size: 8  # 每个步长对应的特征数量
  num_channels: [32, 64, 128, 256]  # 卷积通道数
  kernel_size: 3  # 卷积核大小
  dropout: 0.2  # 丢弃率
  output_size: 1  # 预测未来 n 个时刻数据
  epochs: 10  # 迭代轮数
  batch_size: 32  # 批次大小
  learning_rate: 3e-4  # 学习率
  best_loss: 0  # 记录损失
  model_name: "TCN"  # 模型名称
  save_path: f"saved_models/{model_name}.pth"
Attention:
  data_path: "dataset/wind_dataset.csv"
  timestep: 20  # 时间步长，就是利用多少时间窗口
  feature_size: 1  # 每个步长对应的特征数量
  num_heads: 1  # 注意力机制头的数量
  output_size: 1  # 预测未来 n 个时刻数据
  epochs: 10  # 迭代轮数
  batch_size: 32  # 批次大小
  learning_rate: 3e-4  # 学习率
  best_loss: 0  # 记录损失
  model_name: "Attention"  # 模型名称
  save_path: f"saved_models/{model_name}.pth"
CNN_Attention:
  data_path: "data/wind_data.csv"
  timestep: 20  # 时间步长，就是利用多少时间窗口
  feature_size: 1  # 每个步长对应的特征数量
  num_heads: 1  # 注意力机制头的数量
  out_channels: [10, 20, 30]  # 卷积层输出通道
  output_size: 1  # 预测未来 n 个时刻数据
  epochs: 50  # 迭代轮数
  batch_size: 16  # 批次大小
  learning_rate: 1e-5  # 学习率
  best_loss: 0  # 记录损失
  model_name: "CNN-Attention"  # 模型名称
  save_path: f"saved_models/{model_name}.pth"
LSTM_Attention:
  data_path: "dataset/wind_dataset.csv"
  timestep: 1  # 时间步长，就是利用多少时间窗口
  feature_size: 1  # 每个步长对应的特征数量
  num_heads: 1  # 注意力机制头的数量
  num_layers: 2  # 网络的层数
  hidden_size: 64  # 网络隐藏层大小
  output_size: 1  # 预测未来 n 个时刻数据
  epochs: 50  # 迭代轮数
  batch_size: 16  # 批次大小
  learning_rate: 1e-5  # 学习率
  best_loss: 0  # 记录损失
  model_name: "LSTM-Attention"  # 模型名称
  save_path: f"saved_models/{model_name}.pth"
Transformer:
  data_path: "dataset/wind_dataset.csv"
  timestep: 20  # 时间步长，就是利用多少时间窗口
  feature_size: 1  # 每个步长对应的特征数量
  num_layers: 2  # LSTM 层数
  hidden_size: 256  # 隐藏层大小
  transformer_num_layers: 1  # transformer 层数
  output_size: 1  # 预测未来 n 个时刻数据
  epochs: 10  # 迭代轮数
  batch_size: 32  # 批次大小
  learning_rate: 3e-4  # 学习率
  best_loss: 0  # 记录损失
  model_name: "Transformer"  # 模型名称
  save_path: f"saved_models/{model_name}.pth"
Informer:
  data_path: "dataset/wind_dataset.csv"
  timestep: 20  # 时间步长，就是利用多少时间窗口
  feature_size: 8  # 每个步长对应的特征数量
  output_size: 1  # 预测未来 n 个时刻数据
  epochs: 10  # 迭代轮数
  batch_size: 512  # 批次大小
  learning_rate: 3e-4  # 学习率
  best_loss: 0  # 记录损失
  model_name: "Informer"  # 模型名称
  save_path: f"saved_models/{model_name}.pth"